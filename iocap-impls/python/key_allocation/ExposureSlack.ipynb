{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459cf335-6144-4d77-bd52-c003a21ede53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import exposure_slack_sims as sim\n",
    "from dataclasses import dataclass\n",
    "from typing import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23eb126f-cdda-4f76-af61-ed4486690259",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETHERNET_MTU_STANDARD = 1500\n",
    "ETHERNET_MTU_JUMBO = 9000\n",
    "\n",
    "KiB = 2 ** 10\n",
    "MiB = 2 ** 20\n",
    "GiB = 2 ** 30\n",
    "\n",
    "ms = 1/1_000\n",
    "us = 1/1_000_000\n",
    "\n",
    "class KeyAmountCalcScheme:\n",
    "    def n_in_queue_keys(self, queue_len: int, elem_size_b: int) -> int: ...\n",
    "    def n_buffer_keys(self) -> int: ...\n",
    "\n",
    "class ExactKeyAmountPerQueue(KeyAmountCalcScheme):\n",
    "    # Must include at least one 'buffer' key e.g. for a queue\n",
    "    # AAAAAAABBBBBBBCCCCCCCDD    <- new elements can be created in D  \n",
    "    # |-----||-----||-----||-----|\n",
    "    # If you may be consuming OoO, it may be wise to keep multiple buffer keys e.g.\n",
    "    # AAABBBBBCCCCCCCDDDDDDDE    <- A not consumed yet, B not consumed yet, C and D are full, would need E\n",
    "    \n",
    "    def __init__(self, n_in_queue_keys: int, n_buffer_keys: int=1):\n",
    "        self.in_queue = n_in_queue_keys\n",
    "        self.buffer = n_buffer_keys\n",
    "\n",
    "    def n_in_queue_keys(self, queue_len: int, elem_size_b: int) -> int:\n",
    "        return self.in_queue\n",
    "    def n_buffer_keys(self) -> int:\n",
    "        return self.buffer\n",
    "\n",
    "class KeyAmountTargetingOverheadPerQueue(KeyAmountCalcScheme):\n",
    "    targeted_overhead_b: int\n",
    "\n",
    "    def __init__(self, targeted_overhead_b: int, n_buffer_keys: int=1):\n",
    "        self.targeted_overhead_b = targeted_overhead_b\n",
    "        self.buffer = n_buffer_keys\n",
    "\n",
    "    def n_in_queue_keys(self, queue_len: int, elem_size_b: int) -> int:\n",
    "        # overhead = (queue_len // n_queue_keys) * elem_size_b\n",
    "        # overhead / elem_size_b = queue_len // n_queue_keys\n",
    "        # n_queue_keys / queue_len = elem_size_b / overhead\n",
    "        # n_queue_keys = queue_len * elem_size_b / overhead\n",
    "        n_keys = queue_len * elem_size_b // self.targeted_overhead_b\n",
    "        print(f\"KeyAmountTargetingOverhead selecting {n_keys} in-queue keys\")\n",
    "        if self.targeted_overhead_b <= elem_size_b:\n",
    "            print(f\"WARNING: Selecting a targeted overhead of {self.targeted_overhead_b} for a simulation with elem size {elem_size_b} results in overprovisioning keys\")\n",
    "        return n_keys\n",
    "    def n_buffer_keys(self) -> int:\n",
    "        return self.buffer\n",
    "\n",
    "@dataclass\n",
    "class SimConfig:\n",
    "    queue_len: int\n",
    "    elem_size_b: int\n",
    "    n_queues: int\n",
    "    key_amount: KeyAmountCalcScheme\n",
    "    \n",
    "    def run(self, ticks=10_000) -> Tuple['SimConfig', sim.SaturatedQueueStats]:\n",
    "        n_in_queue_keys = self.key_amount.n_in_queue_keys(self.queue_len, self.elem_size_b)\n",
    "        n_buffer_keys = self.key_amount.n_buffer_keys()\n",
    "        stats = sim.SaturatedQueue(\n",
    "            key_alloc=sim.RoundRobinLimitKeyAllocator(num_keys=(n_in_queue_keys + n_buffer_keys), key_limit=max(1, math.ceil(self.queue_len / n_in_queue_keys))),\n",
    "            elem_consume=sim.FifoElementConsumer(num_elems=self.queue_len),\n",
    "        ).run(ticks)\n",
    "        return (self, stats)\n",
    "\n",
    "def log_stats(sim_stats: Tuple[SimConfig, sim.SaturatedQueueStats]):\n",
    "    sim.log_stats(sim_stats[1], elem_size=sim_stats[0].elem_size_b, n_queues=sim_stats[0].n_queues)\n",
    "\n",
    "def run_and_log(sim: SimConfig):\n",
    "    sim_stats = sim.run()\n",
    "    log_stats(sim_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef3fbc-80a2-4968-b12b-dc86c8fce809",
   "metadata": {},
   "source": [
    "# NVMe\n",
    "\n",
    "Consider 1us latency. If that's accurate (TODO source): 1us = 1 millionth of a second = 1 per MHz = 1000 cycles on a 1GHz, 5000 cycles on a 5GHz. \n",
    "DRAM DDR5 latency is 10-20ns but a) that's latency at the controller I think and b) that's pipelined, it's not like you only have 50-100 DRAM writes between issuing the NVMe request and it coming back.\n",
    "**Unfortunately this is not accurate - haasWhatModernNVMe2023 measures latency at 60-100us**.\n",
    "\n",
    "Anecdotal sources cap out at 64 and claim it's hard to hit that randomly anyway:\n",
    "- https://uk.pcpartpicker.com/benchmarks/methodology/storage/\n",
    "- https://www.tomshardware.com/features/ssd-benchmarks-hierarchy\n",
    "    > We've sorted by the random QD1 IOPS results for the tables — the geometric mean of both the read and write IOPS, to be precise. This is one of the more realistic representations of overall SSD performance, even if it's a synthetic test, as it's difficult to game the system. Lots of manufacturers will test random IO performance at queue depths of 32 or even 256 because that makes everything look much faster, but in the real world, random queue depths are mostly at QD1 and almost never go beyond QD4.\n",
    "\n",
    "These anecdotal setups use 4KiB for random and 1MiB for sequential I/O.\n",
    "\n",
    "Sequential I/O is more likely to saturate the queue????? so assume 1MiB.\n",
    "\n",
    "## Dell\n",
    "\n",
    "<https://infohub.delltechnologies.com/en-us/l/an-in-depth-overview-of-nvme-and-nvme-of/queue-implementations/#:~:text=The%20ability%20of%20NVMe%20to%20create%20many,queue%20depths%20ranging%20from%2032%20to%202%2C048.>\n",
    "\n",
    "> The ability of NVMe to create many queues with large queue depths ensures that sufficient commands can be queued to use compute resources efficiently and use all available fabric bandwidth fully. While NVMe supports over 65,000 queues with queue depth support over 65,000 commands per queue, the typical NVMe implementation attempts to allocate a maximum of one queue pair per host CPU core. This results in implementations with normal ranges being from four to 256 queues with queue depths ranging from 32 to 2,048.\n",
    "\n",
    "## zouDirectNVMHardwareacceleratedNVMe2022\n",
    "\n",
    "> NVMe supports 64K pairs of I/O queues, each with 64K entries, which cannot fit in on-chip memories. However, in most use cases, a very shallow I/O queue is sufficient. Intel conducted a series of SSD benchmarking with different workloads [9] and got a conclusion that the NVMe performance asymptotically saturates as the queue depth increases. Our experiments also verify that the throughput slightly changes when the I/O queue depth grows larger than 64, which will be further explained and validated through approximate mathematical modeling in Section 7.1.\n",
    "\n",
    "Note that this is inside their own custom system on an FPGA, so it's only good as a ballpark\n",
    "\n",
    "## haasWhatModernNVMe2023\n",
    "\n",
    "2023 paper showing having ~3000 requests outstanding on 8 NVMe SSDs saturates them in Figure 4, hence we can likely assume 3000/8 = 375 saturates one 2023 NVMe SSD.\n",
    "\n",
    "\n",
    "## intelPerformanceBenchmarkingPCIe2015\n",
    "\n",
    "2015 whitepaper showing NVMes of the time used on the order of 50-100 queue depth\n",
    "\n",
    "## harrisFreeBSDNVMExpress2018\n",
    "\n",
    "As of 2018 in vanilla FreeBSD each queue has 128 outstanding items."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657c597a-5c7e-4933-95c7-cb970eda904d",
   "metadata": {},
   "source": [
    "# Netflix FreeBSD NVMe Workload\n",
    "\n",
    "<https://freebsdfoundation.org/wp-content/uploads/2018/08/FreeBSD-and-NVM-Express.pdf> from 2018 states Netflix uses MAXPHYS=1MiB, and that in vanilla FreeBSD each queue has 128 outstanding items.\n",
    "\n",
    "<https://lists.freebsd.org/pipermail/freebsd-arch/2020-November/020144.html> from 2020 states \"Netflix has run MAXPHYS of 8MB for years\"\n",
    "\n",
    "FreeBSD 13 bumped MAXPHYS to 1MiB anyway.\n",
    "\n",
    "https://netflixtechblog.com/serving-100-gbps-from-an-open-connect-appliance-cdb51dda3b99 states Netflix introduced a new type of mbuf that bundles 24x 4KiB pages = 96KiB, but this serves 1MiB \"HTTP range request\"s.\n",
    "\n",
    "<https://papers.freebsd.org/2021/eurobsdcon/gallatin-netflix-freebsd-400gbps/>\n",
    "- 400Gb/s\n",
    "    - AMD EPYC 7502P (“Rome”) 32 cores\n",
    "    - 2x Mellanox ConnectX-6 Dx\n",
    "        - Gen4 x16, 2 full speed 100GbE ports per NIC\n",
    "            - 4 x 100GbE in total\n",
    "    - 18x WD SN720 NVME\n",
    "    - Can't assume a given NVMe drive is dedicated to a given core.\n",
    "    - \n",
    "<https://papers.freebsd.org/2022/eurobsdcon/gallatin-the_other_freebsd_optimizations-netflix/>\n",
    "- 800Gb/s\n",
    "    - 2x AMD EPYC 7713 64c / 128t (128c / 256t total)\n",
    "    - 4x Mellanox ConnectX-6 Dx (8x 100GbE ports) NICs\n",
    "    - 16x Intel Gen4 x4 14TB NVME\n",
    "\n",
    "TODO SHOULD WATCH THOSE PRESENTATIONS NOT JUST READ SLIDES\n",
    "\n",
    "NO INFO ON NETFLIX QUEUE SIZES, guessing 128?\n",
    "Worth noting that with multiple NVMe devices you'd get one per device per core!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c934817-ddb7-4277-80a6-a41ea12627fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_QUEUE_LEN=128\n",
    "\n",
    "def netflix_freebsd_400Gbps_nvme_config(key_amount: KeyAmountCalcScheme) -> SimConfig:\n",
    "    N_CORES=32\n",
    "    N_NICS=2\n",
    "    N_NIC_PORTS=N_NICS * 2\n",
    "    N_NVMES=18\n",
    "    return SimConfig(\n",
    "        elem_size_b= 8 * MiB,\n",
    "        n_queues=N_NVMES * N_CORES, # This could be reduced, if we think are aren't doing NUMA hops from NVMe->core, but it really seems like they're still hopping\n",
    "        queue_len=UNK_QUEUE_LEN,\n",
    "        key_amount=key_amount,\n",
    "    )\n",
    "\n",
    "def netflix_freebsd_400Gbps_network_config(key_amount: KeyAmountCalcScheme) -> SimConfig:\n",
    "    N_CORES=32\n",
    "    N_NICS=2\n",
    "    N_NIC_PORTS=N_NICS * 2\n",
    "    N_NVMES=18\n",
    "    return SimConfig(\n",
    "        elem_size_b= 1 * MiB, # 1MiB?\n",
    "        n_queues=N_NIC_PORTS * N_CORES, # at least\n",
    "        queue_len=UNK_QUEUE_LEN,\n",
    "        key_amount=key_amount,\n",
    "    )\n",
    "\n",
    "def netflix_freebsd_800Gbps_nvme_config(key_amount: KeyAmountCalcScheme) -> SimConfig:\n",
    "    N_CORES=128\n",
    "    N_NVMES=16\n",
    "    return SimConfig(\n",
    "        elem_size_b= 8 * MiB,\n",
    "        n_queues=N_NVMES * N_CORES,\n",
    "        queue_len=UNK_QUEUE_LEN,\n",
    "        key_amount=key_amount,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0f226b-c9df-4de3-9984-899a8383cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Runtime\t10000\n",
      "TicksBlocked\t0\n",
      "MaxConsumedNotFreed\t7\n",
      "MaxConsumedNotFreedBPerQ\t56.00 MB\n",
      "NQueues                 \t576\n",
      "MaxConsumedNotFreedTotalB\t31.50 GB\n",
      "QuartConsumedNotFreed\t[0.0, 1.0, 2.0, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "run_and_log(netflix_freebsd_400Gbps_nvme_config(ExactKeyAmountPerQueue(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bac5fe5-c6c6-4dc8-a937-c4a9cb3d1bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Runtime\t10000\n",
      "TicksBlocked\t0\n",
      "MaxConsumedNotFreed\t3\n",
      "MaxConsumedNotFreedBPerQ\t24.00 MB\n",
      "NQueues                 \t576\n",
      "MaxConsumedNotFreedTotalB\t13.50 GB\n",
      "QuartConsumedNotFreed\t[0.0, 0.0, 1.0, 1.0, 1.5, 2.0, 2.0, 3.0, 3.0]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "run_and_log(netflix_freebsd_400Gbps_nvme_config(ExactKeyAmountPerQueue(32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22696ea4-c331-496f-aae1-094a1c8b64de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyAmountTargetingOverhead selecting 102 in-queue keys\n",
      "-------\n",
      "Runtime\t10000\n",
      "TicksBlocked\t0\n",
      "MaxConsumedNotFreed\t1\n",
      "MaxConsumedNotFreedBPerQ\t8.00 MB\n",
      "NQueues                 \t576\n",
      "MaxConsumedNotFreedTotalB\t4.50 GB\n",
      "QuartConsumedNotFreed\t[0.0, 0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "run_and_log(netflix_freebsd_400Gbps_nvme_config(KeyAmountTargetingOverheadPerQueue(10 * MiB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc3e8b5-e497-4c3a-a21a-07eb6ba6fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_queue_size(rate_bytes_per_second, block_size_bytes, latency_milliseconds):\n",
    "    # My guestimating is based on the following logic, which is similar to TCP Bandwidth Delay Product and RWIN.\n",
    "    # Consider an NVMe disk. It receives a trivial amount of data to request a transfer, then either has to transfer data to or from host memory.\n",
    "    # Assume for now that it is only transferring data into host memory (the host is reading).\n",
    "    # Ideally, there should be enough room in the host queue for the NVMe to fill the queue *without stopping*, ...\n",
    "    # ... in the TCP case it would have to stop after transmitting RWIN packets if it didn't receive an ACK for the first packet,\n",
    "    # so you need to size RWIN to have enough data to saturate the network while waiting for a packet to send to the other side, and for the other side to send an ACK back.\n",
    "    # i.e. RWIN needs to have enough capacity for bandwidth * (t_send + t_ack), which is usually bandwidth * RTT\n",
    "    # but in the NVMe case it just has to put stuff in *another queue* and ring the doorbell.\n",
    "    # print((rate_bytes_per_second * latency_milliseconds) / 1000.0)\n",
    "    return ((rate_bytes_per_second * latency_milliseconds) / 1000.0) // block_size_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8a7f324-f2b0-4fdd-8946-4bb99386749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Experiment: PCIe4 NVMes claim 5000 MB/s, paging at 4KiB, and microsecond-ish latencies\n",
    "one_microsecond_in_milliseconds = 1.0 / 1000.0\n",
    "nvme_read_queue_size = estimate_queue_size(5000 * 1000 * 1000, 4 * KiB, one_microsecond_in_milliseconds)\n",
    "print(nvme_read_queue_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b008c1e8-9d3d-4cb4-aff0-fa4957eb502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "nvme_of_tcp_read_queue_size = estimate_queue_size(800 * 1000 * 1000 * 1000 / 8, 1 * MiB, 30.0 / 1000.0)\n",
    "print(nvme_of_tcp_read_queue_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "113f3c15-c9eb-4bd1-b141-e5d90cd7dfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Runtime\t10000\n",
      "TicksBlocked\t0\n",
      "MaxConsumedNotFreed\t4\n",
      "MaxConsumedNotFreedBPerQ\t4.00 MB\n",
      "NQueues                 \t8\n",
      "MaxConsumedNotFreedTotalB\t32.00 MB\n",
      "QuartConsumedNotFreed\t[0.0, 0.8, 1.0, 1.6, 2.0, 2.4, 3.0, 3.2, 4.0]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "run_and_log(SimConfig(\n",
    "    elem_size_b=1 * MiB,\n",
    "    n_queues=8, # one NVMe * 8 cores\n",
    "    queue_len=64,\n",
    "    key_amount=ExactKeyAmountPerQueue(15),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b07cb-5879-45e3-8a5c-568aba71af63",
   "metadata": {},
   "source": [
    "# High-perfomrance networking\n",
    "\n",
    "https://www.nvidia.com/content/dam/en-zz/Solutions/networking/ethernet-adapters/connectX-6-dx-datasheet.pdf\n",
    "- 200Gb/s = 25GB/s\n",
    "\n",
    "NVIDIA recommends jumbo 9000B ethernet packets\n",
    "- https://docs.nvidia.com/grace-perf-tuning-guide/optimizing-io.html\n",
    "- but if we're calculating worst case longest queue length, it's actually better to assume the packets are smaller\n",
    "\n",
    "Windows driver caps out at 4096 for each of RX/TX queue\n",
    "- https://docs.nvidia.com/networking/display/winof2v237/configuring+the+driver+registry+keys\n",
    "\n",
    "Latency = ??\n",
    "\n",
    "Rolf paper figure 5 approximates it at 1-1.6us\n",
    "\n",
    "But here the latency needs to actually be the latency of the link, too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f3db72-16d6-48a2-8c04-af7d2bf315bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_queue_size(25 * 1000 * 1000 * 1000, 1500, one_microsecond_in_milliseconds * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497f72c-f9b6-4000-a4d9-6fb4dcf41f40",
   "metadata": {},
   "source": [
    "# Consumer networking\n",
    "\n",
    "https://www.intel.com/content/www/us/en/products/docs/wireless/2-4-vs-5ghz.html\n",
    "\n",
    "Worst case: saturated ethernet or 5GHz wifi, around a gigabit = 128MBytes/s.\n",
    "\n",
    "45Mbit required for GeForce Now:  https://www.nvidia.com/en-gb/geforce-now/system-reqs/#windows-pc\n",
    "100ms worst case ping???? If you're sending 128MBytes/s to japan???\n",
    "\n",
    "nicholsControllingQueueDelay2012 describes CoDel, which appears to have been accepted as *the* dynamic TCP sizing algorithm for buffer bloat?\n",
    "\n",
    "Assume no jumbo packets, AFAIK they require a lot of network config to work without breaking everything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f77a40-d32b-4dce-b46f-8f2857441b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8533.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_queue_size(128 * 1000 * 1000, 1500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1f212-8021-41d7-a0d8-c165ed684bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "761843a2-ee6a-486c-9b46-c587dd4c9766",
   "metadata": {},
   "source": [
    "# Key Lifetime Justification\n",
    "\n",
    "## haasWhatModernNVMe2023\n",
    "\n",
    "2023 paper showing having ~3000 requests outstanding on 8 NVMe SSDs saturates them in Figure 4, hence we can likely assume 3000/8 = 375 saturates one 2023 NVMe SSD.\n",
    " \n",
    "- Max Queue Length = ~512 for max performance\n",
    "- Latency = 60-100us, see Fig 3c\n",
    "- Throughput has multiple sources: around 1million per SSD maximum stated in introduction, in practice closer to (3.75M IOPS/8 SSDs = 470k) for LeanStore in fig 1. Slower => longer key lifetime\n",
    "\n",
    "So, consider a case where a NVMe drive has been configured for high-throughput (512 queue length) but is being used slowly in a way to maximize key lifetime.\n",
    "There are only two keys, the bare minimum required which enforces each key is one whole queue length, and each transaction arrives just before the previous one is finished.\n",
    "That implies ops-per-second = 1/latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c026f49-f67b-4b5e-b2fc-5e7a00fddbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.10ms\n"
     ]
    }
   ],
   "source": [
    "def max_key_lifetime(*, elems_per_key, initial_latency_s, elems_per_s):\n",
    "    return initial_latency_s + elems_per_key / elems_per_s\n",
    "\n",
    "def print_ms(seconds):\n",
    "    print(f\"{seconds*1000:.2f}ms\")\n",
    "\n",
    "NVME_MAX_QUEUE_LENGTH = 500\n",
    "NVME_LATENCY = 100 * us\n",
    "print_ms(max_key_lifetime(\n",
    "    elems_per_key = NVME_MAX_QUEUE_LENGTH,\n",
    "    initial_latency_s = NVME_LATENCY,\n",
    "    elems_per_s = 1 / NVME_LATENCY,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743d721-0739-4fbf-bcf9-d9f2cdcb5d1d",
   "metadata": {},
   "source": [
    "## patelSplitwiseEfficientGenerative2024\n",
    "\n",
    "Table IV shows time-between-tokens are ~50ms, BUT time to first token is up to 200ms. In practice this sort of thing should be uploaded to the GPU anyway...\n",
    "Even local models https://arxiv.org/pdf/2410.08391v1 (WITHDRAWN) take waay longer (multiple seconds) to generate tokens. Prudent to use the pool model here.\n",
    "\n",
    "## Hypothetical task-based architectures\n",
    "\n",
    "### GPUs\n",
    "\n",
    "Consider a GPU rendering a real-time interactive video game at 30 frames per second (33ms). In these cases the CPU and GPU typically run in parallel, with the CPU preparing data for one frame as the GPU is rendering the current frame. If the CPU exposed each frame's data all at once as a single task, (which would imply it's all CPU-resident persistent mappings, which is unlikely) the memory would be exposed for ~33ms. This also assumes the GPU driver exposes that memory at a time very close to frame computation start, such as command buffer build time.\n",
    "\n",
    "\n",
    "### High-res video decode\n",
    "\n",
    "In a similar fashion high resolution video decoding may have dedicated accelerators, which would have 42ms to decode each frame of video at the standard 24fps cinematic frame rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
